[#ipc]
= IPC

Hubris IPC (inter-process communication) provides a mechanism for communicating
between tasks. It's designed to be easy to reason about, and to work well with
Rust's ownership model.

This chapter takes a high-level look at IPC in Hubris, which can be broken apart
into four pieces:

- How to send messages,
- How to receive and handle messages,
- How those two bits interact with task restarts, and
- Notifications, an alternative lighter-weight IPC mechanism.

In practice, most code you write on Hubris will be using various abstractions or
wrapper libraries that obscure the specifics of IPC. However, we think it's
important to understand what's really going on, even if you choose to mostly use
it through a library -- and besides, somebody has to write those libraries, and
that could easily be you. So in this chapter, we'll peel back the abstractions
and deal in the raw IPC operations provided by the kernel.

NOTE: IPC is technically a misnomer, since we don't have what most folks think
of as "processes" (the P in IPC). But hey. Inter-Task Communication just isn't
as catchy.

== Synchronous IPC basics

Hubris IPC is based around _synchronous messaging._ Communication between tasks
consists of sending and receiving messages. Sending a message always blocks the
sender, waiting for a reply. A task that asks to receive a message will block
if no messages are available. In the common case, the sequence is

1. The recipient task, B, finishes doing something else and asks to receive.
2. It is blocked by the kernel, allowing other tasks to run.
3. The sending task, A, prepares a message to B and asks to send.
4. Task A is marked as blocked and task B is resumed to process the message.

When two tasks align in this way, a _message transfer_ occurs. This is an
action performed by the kernel on behalf of the two tasks. During a message
transfer, the kernel copies a small data payload (the _message_) from the
sender to the recipient, and provides the recipient with some metadata it can
use to process the message.

This style of messaging is also called a _rendezvous_ or _handoff,_ because both
tasks need to arrange to "meet" in the right state for the message transfer to
happen.

NOTE: In case you're concerned that all messages are small, you will be relieved
to know that we have a mechanism for dealing with that; if you want to skip
ahead, see <<leases>>.

=== Why synchronous?

This is not the only way to design an IPC system. The main alternatives involve
some sort of asynchrony or queueing, where a task can issue several messages and
then do other work while waiting for responses. This can be useful, so it's
worth asking why Hubris is fully synchronous.

It turns out that synchronous IPC has significant advantages for systems like
Hubris.

**It's fast.** Synchronous IPC can be implemented using a single message copy
from sender to recipient. The message need only be read once. This reduces the
number of clock cycles required for a message transfer. (The operating system
that originally made this point was L4, and Hubris's IPC mechanism is directly
inspired by L4's.)

**There are no queues to size.** Asynchronous systems usually queue messages in
kernel memory, which means there are now kernel-managed queues that must have
their resources accounted for. In extreme cases, this can allow a single task to
exhaust the kernel's memory with impunity by expanding queues to the breaking
point -- which isn't great for reliability. Many systems that try to avoid that
problem do so by allowing the user to impose size limits on queues, which then
become another thing users need to tune carefully for best performance. With
synchronous rendezvous messaging, we avoid having queues in the first place.

**It limits the power of any single task.** A Hubris task can either be doing
local work, or sending one message. It can't, for instance, spam every other
task in the system with messages simultaneously, or set up hundreds of messages
to a single task. This avoids "fault amplification" scenarios where a bug in one
task cascades into a much larger problem, as in a denial-of-service attack.

**It lets message recipients reason about what their callers are doing.** When
your task is processing a message, it knows that the sending task is parked
waiting for a reply.footnote:[Or, that the task has been restarted by a debug
tool or supervision policy, but those don't make this point invalid for reasons
we'll discuss later.] It also knows that, if it goes to receive a _second_
message before replying, that message won't be from the same task. This allows
message recipients to coordinate when senders run and don't run, which is useful
for implementing mutual exclusion, as we'll discuss later.

Finally, **synchronous IPC makes the system much easier to think about.** Each
task operates as a synchronous state machine, with IPC operations appearing as
atomic and predictably ordered. Larger systems composed out of tasks can be
easily halted and inspected to see who's waiting on who. Lots of potential race
conditions are eliminated. We've found this point to be hugely important during
our early experience with the system.

NOTE: You may be wondering about deadlocks in synchronous IPC. We avoid IPC-level deadlocks and priority inversion by imposing rules around messaging and task priority. This makes IPC-level deadlock impossible, but of course you can still write software that deadlocks if you try. More on this in the section <<uphill-send>>.

== Sending messages

To simply consume IPC services implemented by others, there's only one operation
you need to consider: <<sys_send,`send`>>. `send` operates a lot like a function
call:

- It takes some arguments,
- It returns some results,
- It pauses your code (the caller) until it returns.

To use `send`, you specify the task you're sending to, and an *operation code*
that tells that task what operation you're trying to invoke. (A given API will
document which operation codes are used.)

In Rust, `send` has the following signature:

[source,rust]
----
fn sys_send(
    target: TaskId,       // <1>
    operation: u16,       // <2>
    outgoing: &[u8],      // <3>
    incoming: &mut [u8],  // <4>
    leases: &[Lease<'_>], // <5>
) -> (u32, usize);        // <6>
----
<1> The `TaskId` of the task you're trying to contact.
<2> The _operation code_ giving the operation you're requesting.
<3> The message you wish to send.
<4> A buffer where any message sent in response will be deposited.
<5> Zero or more _leases,_ which we'll discuss below in the section <<leases>>.
<6> Returns a `u32` giving the _response code_ and a `usize` saying how many
    bytes were written into `incoming`. These will both be described below in
    the section <<response-codes>>.

The simplest case for `send` is when you're sending a small payload (say, a
`struct`) and receiving a small response (say, another `struct`), and the
recipient/callee is already blocked waiting for messages:

1. Your task invokes `send`, providing the `target` (recipient task) and
   `operation`, as well as the message to send (`outgoing`) as a `&[u8]`. It
   also provides a buffer where the response should be deposited (`incoming`),
   as a `&mut [u8]`.

2. The kernel notices that the recipient is waiting for messages, and directly
   copies your message (operation code and payload data) from your task's
   memory into the callee's.

3. The kernel then marks your task as blocked waiting for reply, and unblocks
   the recipient, informing it that a message has arrived.

4. The recipient does some work on your behalf.

5. It then uses the `reply` operation to send a response message back. (`reply`
   will be covered soon in the section <<recv-and-reply>>).

6. The kernel copies the response message into the `incoming` buffer your task
   provided, makes ``send``'s two result values available to your task, and
   marks your task as runnable.

If the recipient is _not_ already waiting for messages when your task tries to
send, the process is similar, except that your task may be blocked waiting to
send for an arbitrary period between steps 1 and 2.

Note that the kernel doesn't _interpret_ either the operation code or the
message data flowing between tasks. Any meaning there is defined by the
application. The kernel looks only at the task IDs, and copies the rest blindly
from place to place.

TIP: The Hubris source code sometimes refers to operation codes as "selectors"
or "discriminators," because Cliff can't make up his mind on which metaphor to
use.

[#response-codes]
=== Response codes and `Result`

From the caller's perspective, `send` deposits any returned data into the
`incoming` buffer given to `send`. But it also returns two integers:

- A *response code.*
- The *length* of data deposited in the return buffer.

The response code, like the operation code, is largely application-defined, but
it's _intended_ to be used to distinguish success from error. Specifically, `0`
meaning success, and non-zero meaning error.

The length gives the number of bytes in the `incoming` buffer that have been
written by the kernel and are now valid. This happens no matter what the
response code is, so an operation can return detailed data even in the case of
an error.

This scheme is specifically designed to allow IPC wrapper functions to translate
the results of `send` into a Rust `Result<T, E>` type, where `T` is built from
the data returned on success, and `E` is built from the combination of the
non-zero response code and any additional data returned. Most wrappers do this.

NOTE: While response codes are _mostly_ up to the application, there is a class
of non-zero response codes used by the kernel to indicate certain failure cases
related to crashing tasks. These are called "dead codes," and will be covered
later, in the section <<death>>. Applications should choose their response codes
to avoid colliding with them. Fortunately, they're very large numbers, so if
applications start their errors from 1 they should be safe.

=== Message size limits

When a message transfer happens, the kernel diligently copies the message data
from one place to another. This operation is uninterruptible, so it may delay
processing of interrupts or timers. To limit this, we impose a maximum length on
messages, currently 256 bytes.

TIP: If you need to move more data than this, you can use the "lease" mechanism,
described in the next section.

[#leases]
=== Lending out memory

Sending and receiving small structs by copy is a good start, but what if you
want something more complex? For example, how do you send 1kiB of data to a
serial port, or read it back, when messages are limited to 256 bytes?

The answer is the same as in Rust: when you call the operation, you loan it some
of your memory.

Any `send` operation can include *leases*, which are small descriptors that tell
the kernel to allow the _recipient of the message_ to access parts of the
_sender's_ memory space. Each lease can be read-only, write-only, or read-write.
While the sender is waiting for a reply, the recipient has exclusive control of
the leased memory it is borrowing. If the caller resumes (generally after
`reply`, but also possible in some corner cases involving task supervision) the
leases are reliably and atomically _revoked_.

This means it's safe to lend out any memory that the caller can safely access,
including memory from the caller's stack.

This property also means that lending data can be expressed, in Rust, as simple
`&` or `&mut` borrows, and checked for correctness at compile time.

Each `send` can include up to 255 leases (currently -- this number may be
reduced because what on earth do you need that many leases for).

On the recipient side, leases are referred to by index (0 through 255), and an
IPC operation would typically declare that it needs certain arguments passed as
leases in a certain order. For instance, a simple serial write operation might
expect a single readable lease giving the data to send, while a more nuanced I2C
operation might take a sequence of readable and writable leases.

TIP: An operation can also take a _variable_ number of leases and use this to
implement scatter-gather. It's up to the designer of the API.

=== Making this concrete

Let's sketch a concrete IPC interface, to get a feeling for how the various
options on `send` fit together. Imagine a task that implements a very simple
streaming data access protocol consisting of two functions (written as in Rust):

```rust
fn read(fd: u32, buffer: &mut [u8]) -> Result<usize, IoError>;
fn write(fd: u32, buffer: &[u8]) -> Result<usize, IoError>;

enum IoError {
    Eof = 1,
    ContainedBobcat = 2,
}
```

These are basically POSIX read and write, only expressed in Rust style.

A concrete mapping of these operations to IPCs might go as follows.

**Read.** Operation code 0.

- Message is a four-byte struct containing `fd` as a little-endian `u32`. Borrow
  0 is `buffer` and must be writable.
- Data will be written to a prefix of borrow 0, starting at offset 0.
- On success, returns response code 0 and a four-byte response, containing the
  bytes-read count as a little-endian `u32`.
- On failure, returns a non-zero response code that maps to an `IoError`, and a
  zero-length response message.

**Write.** Operation code 1.

- Message is a four-byte struct containing `fd` as a little-endian `u32`. Borrow
  0 is `buffer` and must be readable.
- Data will be taken from a prefix of borrow 0, starting at offset 0.
- On success, returns response code 0 and a four-byte response, containing the
  bytes-written count as a little-endian `u32`.
- On failure, returns a non-zero response code that maps to an `IoError`, and a
  zero-length response message.

NOTE: Either of these operations could be altered to also return the number of
bytes read or written in an error case, by making the response non-empty and
changing the `IoError` type in Rust to have data fields.

A very simple IPC stub for the `read` operation might be written as follows.

[source,rust]
----
use userlib::{TaskId, FromPrimitive, sys_send};

#[derive(Copy, Clone, Debug, FromPrimitive)]
enum IoError {
    Eof = 1,
    ContainedBobcat = 2,
}

fn read(task: TaskId, fd: u32, buffer: &mut [u8]) -> Result<usize, IoError> {
    let mut response = [0; 4];
    let (rc, len) = sys_send(
        task,
        0,
        &u32.to_le_bytes(),
        &mut response,
        &[Lease::from(buffer)],
    );
    if let Some(err) = IoError::from_u32(rc) {
        Err(err)
    } else {
        assert_eq!(len, 4);
        Ok(u32::from_le_bytes(&response))
    }
}
----

(`write` would be nearly identical, but with the operation code changed.)

[#recv-and-reply]
== Receiving and handling messages

To write a task that implements some IPC protocol, we need to be able to receive
and handle messages. There are two operations involved on this side:

- <<sys_recv,`recv`>> gets the next pending message, and
- <<sys_reply,`reply`>> unblocks the sender of a message, optionally delivering
  a response data payload.

[source,rust]
----
fn sys_recv_open(
    buffer: &mut [u8],
    notification_mask: u32, // <1>
) -> RecvMessage;

struct RecvMessage {
    pub sender: TaskId,           // <2>
    pub operation: u32,           // <3>
    pub message_len: usize,       // <4>
    pub response_capacity: usize, // <5>
    pub lease_count: usize,       // <6>
}
----
<1> The `notification_mask` is used for a facility we haven't described yet,
which will be covered below in <<notifications>>.
<2> `sender` is the `TaskId` of the task that sent the message. This is provided
    by the kernel and is reliable, i.e. there is no way for a task to lie here.
<3> The operation code sent by the sender. (You might notice that this is 32
bits while the equivalent argument to `send` is only 16. This will _also_ be
explained in the section <<notifications>>.)
<4> Length of the sent message. If this is larger than `buffer.len()`, the
caller sent an over-long message that has been truncated, and you likely want to
return an error.
<5> Number of bytes the caller reserved for receiving your reply.
<6> Number of leases the caller sent you.

[source,rust]
----
fn sys_reply(peer: TaskId, code: u32, message: &[u8]);
----

Note that `sys_reply` cannot fail. This will be unpacked in the next section.

=== Pipelining, out-of-order replies, and reply failure

Hubris does _not_ require that you `reply` before calling `recv` again. You
could instead start an operation, do some bookkeeping to keep track of that
sender, and then `recv` the next, with the intent of replying later. This
allows you to implement a pipelined server that overlaps requests.

Hubris also doesn't require that you `reply` in the same order as `recv`. For
example, in a pipelined server, you might want to promptly `reply` with an error
to a bogus request while still processing others. Or, in a fully asynchronous
server (such as a network stack for something like UDP), you might `reply`
whenever operations finish, regardless of their order.

Hubris doesn't actually require that you `reply`, _ever._ The caller will wait
patiently. This means if you want to halt a task, sending a message to someone
who will never reply is a reasonable technique. Or, a server could halt
malfunctioning callers by never replying (see next section).

What *is* required for `reply` to succeed is that the sender must actually be
blocked in a send _to your task._ If you `reply` to a random task ID that has
never messaged you, the reply will not go through. If the sending task has been
forceably restarted by some supervising entity, the reply will not go through.
Similarly, if an application implements IPC timeouts by forceably unblocking
senders that have waited too long (something you can choose to do), the reply to
the timed-out sender won't go through.

Because the latter two cases (sender timed out, sender rebooted) are expected to
be possible in an otherwise functioning application, and because it isn't clear
in general how a server should handle a behavior error in one of its clients,
the `reply` operation _does not return an error to the server,_ even if it
doesn't go through. The server moves on.

NOTE: This design decision copies MINIX 3, and those folks explained the
decision in much greater detail. See <<herder08ipc>> for details, and
<<shap03vuln>> for motivating history.

=== Handling error cases on receive

Hubris assumes that you mistrust tasks sending you messages, and provides enough
information to detect the following error cases:

- Unknown operation code.
- Incoming message shorter or longer than what you expected, given the operation
  code.
- Wrong number of leases attached for the operation.
- Sender's response buffer too small to accommodate your reply.

Any of these suggest that the sender is confused or malfunctioning. You have a
few options for dealing with these cases:

- Immediately `reply` to the sender with a non-zero response code and
  zero-length message. Even if the sender is sending to the wrong task, the
  convention around non-zero response codes means this is likely to be
  interpreted as an error by the sender.

- *Don't* reply. Leave the sender blocked, and instead notify some sort of
  supervising entity of a potential malfunction. Or, depending on your
  application architecture, just leave them blocked and expect a watchdog timer
  to handle the problem if it matters.

[#open-and-closed-recv]
=== Open and closed receive

`recv` can operate in two modes, called **open receive** and **closed receive**.
The general case that we've been discussing so far is the open receive case.

In a closed receive, the task selects a single other task to receive messages
from. Any other sender will be blocked.

This can be used to implement mutual exclusion. If a client sends a lock request
to a server, for instance, the server could then perform a closed receive and
only accept messages from that client. Other clients could send lock requests,
but they'd queue up, until the first client either sends a "release" message, or
dies (see below).

[#death]
== Death and IPC

Tasks sometimes restart. For instance, the program running in a task may
`panic!` or dereference an invalid pointer, both of which produce a fault
against the task within the kernel. Normally, the supervisor task is expected
to notice this and reinitialize the failed task. When the task is restarted, a
number associated with the task, its _generation,_ is incremented in the kernel.

The `TaskId` type used to designate tasks for IPC includes both a fixed
identifier for the task (its index) and this generation. The generation part of
the `TaskId` is checked on any IPC, and if it doesn't match, the operation will
fail.

This is intended to detect cases where, during an exchange of messages between
two tasks, one restarts and the other doesn't. Thanks to the generation
mechanism, the task that _didn't_ restart will get notified that the other task
_did._ It can then decide how to proceed -- maybe the protocol between them is
stateless, and no action is needed, but often some kind of an init sequence may
be in order.

When an operation fails because of a generation mismatch, it returns a
predictable response code called a "dead code." A dead code has its 24 top bits
set to 1, with the peer's _new_ generation number in the low 8. You can use this
to update your `TaskId` and retry your request, for instance.

The only currently defined IPC operations that can fail in this way are `send`
and the closed version of `receive`. `reply` does not check generations in
keeping with its fire-and-forget philosophy, and the open version of `receive`
doesn't take a `TaskId` at all so there's nothing to check.

It's important to note that a generation mismatch may be detected at several
different points in time:

1. When a message is initially sent.
2. After the sending task has blocked, but before the receiving task has noticed
the message.
3. After the message has been received, but before it's been replied to.

There's currently no way for the sender to distinguish these cases, so, be
prepared for any of them.

[#notifications]
== Notifications: the _other_ IPC mechanism

In addition to synchronous messaging, Hubris also provides a very limited
asynchronous communication mechanism called **notifications**. Notifications are
designed to complement send-receive style IPC, and are intended for different
purposes. Generally, notifications are useful for situations where one might use
interrupts or signals in other systems.

Each task has 32 notification bits, which together form a _notification set_.
These bits can be _posted,_ which means they are written to `true` -- the number
of posts is not tracked. Each posting operation can touch any subset of the
notification bits, which means the post operation is effectively bitwise-OR-ing
a 32-bit mask into the task's notification set (which is exactly how it's
implemented).

Importantly, posting a notification does _not_ interrupt the receiving task's
code -- it is not like a signal handler or asynchronous exception. Instead, the
receiving task finds out about the notifications only when it checks.

Tasks check for notifications by calling `recv` -- the `recv` operation takes an
additional parameter called the _notification mask,_ which is a 32-bit word. Any
1-bits in the notification mask express to the kernel that the task would like
to find out if the corresponding bit in its notification set has been posted
since it last checked.

If any of the requested bits have been posted:

1. The kernel atomically clears the bits that have been noticed (but leaves
others intact),

2. The task immediately returns from `recv` without blocking, and

3. The result of `recv` is a _notification message_ instead of an IPC.

The task can distinguish a notification message from its contents:

- The sender's `TaskId` will be `TaskId::KERNEL`, indicating that the message
  comes from the kernel. Since the kernel never sends messages in any other
  context, any message "from" the kernel is a notification.

- The `operation` field will contain the bits that were posted and matched the
  provided mask. (These are also the bits that the kernel atomically cleared.)

=== What are they good for?

Notifications are used by the kernel to route hardware interrupts to tasks: a
task can request that an interrupt appear as one of its notification bits. (More
on that in the chapter on Interrupts, below.)

Notifications are also used to signal tasks that the deadline they loaded into
their timer has elapsed. (More on that in the chapter on Timers.)

Finally, notifications can be valuable between tasks in an application, as a way
for one task to notify another of an event without blocking. In particular,
notifications are the only safe way for a high-priority server shared by many
clients to signal a single client -- if it used `send` instead, that client
could decide not to `reply`, starving all the other clients.

In developing firmware on Hubris we've found a particular pattern to be useful,
called "pingback." In this pattern, a high-priority shared server (such as a
network stack) has obtained data that it needs to give to one of its clients --
but it can't just `send` the data for the reason described above. One option is
to have all clients forever blocked in `send` to the _server_ until data
arrived, but that keeps the clients from ever doing anything else! Instead, the
server and clients can agree on a protocol where

1. The client ``send``s to the server, to do whatever setup is required (e.g. to
express interest in data from a particular port). The client provides the server
with a _notification set_ that it wants to receive when the event occurs.

2. The server notes this and ``reply``s immediately.

3. The client goes on about its business, periodically checking for the
notifications it requested.

4. When the server receives data, it posts a notification to the client.

5. When the client notices this, it calls back to the server seeking more
information, and providing a writable lease to some memory where the server can
deposit the result.

6. The server receives this message and copies the data over.

This pattern is useful, in part, because it's very tolerant of a defective
client task. If the server posts the notification and the client _never
responds,_ it's no skin off the server's back -- it's still free to continue
serving other clients.
